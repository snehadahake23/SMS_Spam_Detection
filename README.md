# ğŸ“± SMS Spam Detection: ML vs BERT Showdown

<div align="center">

[![Python 3.6](https://img.shields.io/badge/python-3.6+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Machine Learning](https://img.shields.io/badge/ML-Classification-green.svg)]()
[![NLP](https://img.shields.io/badge/NLP-BERT-orange.svg)]()
[![SMOTE](https://img.shields.io/badge/Imbalanced-SMOTE-red.svg)]()

**A comprehensive comparison of traditional ML algorithms and BERT transformer for SMS spam detection**

[Features](#-key-features) â€¢ [Results](#-model-performance) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Dataset](#-dataset)

</div>

---

## ğŸ¯ Project Overview

This project presents a **dual-approach** spam detection system for SMS messages, comparing:

1. ğŸ”§ **Traditional Machine Learning** - 6 algorithms with Bag-of-Words embeddings
2. ğŸ§  **BERT Transformer** - State-of-the-art deep learning with contextual understanding
3. âš–ï¸ **SMOTE Enhancement** - Addressing class imbalance in the dataset

**Result**: Both approaches achieve exceptional **98% accuracy**, demonstrating that well-tuned traditional ML can compete with modern transformers for text classification tasks!

## ğŸ“Š Model Performance

| Model | Embeddings | Accuracy | Rank |
|-------|-----------|----------|------|
| ğŸ¥‡ **BERT** | BERT Tokenizer | **0.98** | #1 |
| ğŸ¥ˆ **LinearSVC** | Bag-of-Words | **0.98** | #1 |
| ğŸ¥‰ **SGD** | Bag-of-Words | **0.98** | #1 |
| ğŸ… **Random Forest** | Bag-of-Words | **0.98** | #1 |
| **Logistic Regression** | Bag-of-Words | 0.97 | #5 |
| **Gradient Boosting** | Bag-of-Words | 0.97 | #5 |
| **Naive Bayes** | Bag-of-Words | 0.95 | #7 |

> ğŸ’¡ **Key Insight**: Four models achieved identical 98% accuracy, with BERT offering superior semantic understanding while traditional models provide faster inference!

## âœ¨ Key Features

<table>
<tr>
<td width="33%">

### ğŸ¤– Multiple Algorithms
- Logistic Regression
- Naive Bayes
- LinearSVC
- Random Forest
- SGD Classifier
- Gradient Boosting

</td>
<td width="33%">

### ğŸ§  Advanced NLP
- BERT Transformers
- Tokenization
- Stopword Removal
- Porter Stemming
- TF-IDF Weighting

</td>
<td width="33%">

### âš–ï¸ Data Balancing
- SMOTE Oversampling
- Handles Imbalanced Data
- Improved Minority Class
- Better Generalization

</td>
</tr>
</table>

## ğŸ“‚ Project Structure

```
ğŸ“¦ spam-detection/
â”‚
â”œâ”€â”€ ğŸ“Š Notebooks/
â”‚   â”œâ”€â”€ Spam.ipynb                     # Part 1: ML algorithms analysis
â”‚   â””â”€â”€ Spam_bert.ipynb                # Part 2: BERT implementation
â”‚
â”œâ”€â”€ ğŸ Scripts/
â”‚   â”œâ”€â”€ clean_data.py                  # Data cleaning & preprocessing
â”‚   â”œâ”€â”€ spam_model.py                  # Standard ML pipeline
â”‚   â”œâ”€â”€ spam_smote_model.py            # ML with SMOTE balancing
â”‚   â”œâ”€â”€ spam_bert.py                   # BERT model training
â”‚   â””â”€â”€ predictions.py                 # Inference & prediction script
â”‚
â”œâ”€â”€ ğŸ’¾ models/
â”‚   â””â”€â”€ spam_best_model.pkl            # Trained model artifacts
â”‚
â”œâ”€â”€ ğŸ“ data/
â”‚   â”œâ”€â”€ spam.csv                       # Raw SMS dataset
â”‚   â””â”€â”€ spam_clean.csv                 # Preprocessed data
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt                # Python dependencies
â””â”€â”€ ğŸ“– README.md
```

## ğŸš€ Installation

### Prerequisites
- Python 3.6 or higher
- pip package manager

### Quick Start

**1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/yourusername/spam-detection.git
cd spam-detection
```

**2ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

**3ï¸âƒ£ Download NLTK Data**
```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```

**4ï¸âƒ£ Verify Installation**
```bash
python clean_data.py
```

## ğŸ’» Usage

### ğŸ“Š Jupyter Notebooks (Recommended for Learning)

**Approach 1: Traditional ML Analysis**
```bash
jupyter notebook Spam.ipynb
```
- Exploratory data analysis
- Feature engineering visualization
- Compare 6 ML algorithms
- Performance evaluation

**Approach 2: BERT Deep Learning**
```bash
jupyter notebook Spam_bert.ipynb
```
- BERT tokenizer implementation
- Fine-tune transformer model
- Compare with traditional methods

---

### âš¡ Command Line Scripts (Production Ready)

#### ğŸ§¹ Step 1: Clean and Preprocess Data
```bash
python clean_data.py
```
**What it does:**
- Loads raw spam.csv dataset
- Removes unnecessary columns
- Cleans text (punctuation, lowercase)
- Removes stopwords
- Applies Porter stemming
- Saves to `spam_clean.csv`

**Output:**
```
(5572, 2)
   Class                                               Text
0      0                           go jurong point crazi avail bugi...
1      0                                         ok lar joke wif oni...
```

---

#### ğŸ¤– Step 2: Train Models

**Option A: Standard ML Pipeline**
```bash
python spam_model.py
```
Trains 6 classifiers with standard train-test split (80/20).

**Option B: SMOTE-Enhanced Pipeline (Recommended)**
```bash
python spam_smote_model.py
```
Uses SMOTE oversampling to handle class imbalance before training.

**Option C: BERT Transformer**
```bash
python spam_bert.py
```
Fine-tunes pretrained BERT model from Hugging Face.

**Expected Output:**
```
              Model     Score
0         LinearSVC  0.982063
1     SGDClassifier  0.981166
2  RandomForestClassifier  0.979372
3  LogisticRegression  0.974895
...
```

---

#### ğŸ¯ Step 3: Make Predictions

```bash
python predictions.py
```

**Interactive Usage:**
```
Type your message:
> Congratulations! You've won a $1000 gift card. Click here to claim.

Your message is spam
```

```
Type your message:
> Hey, are we still meeting for lunch tomorrow?

Your message is not spam
```

## ğŸ”¬ Technical Details

### Data Preprocessing Pipeline

```python
Raw SMS Text
    â†“
Remove Punctuation (re.sub)
    â†“
Lowercase Conversion
    â†“
Tokenization (split by whitespace)
    â†“
Remove Stopwords (NLTK)
    â†“
Stemming (Porter Stemmer)
    â†“
Feature Vectors (CountVectorizer + TF-IDF)
    â†“
Model Training
```

### Approach 1: Traditional ML

**Pipeline Configuration:**
```python
Pipeline([
    ('vect', CountVectorizer(min_df=5, ngram_range=(1, 2))),
    ('tfidf', TfidfTransformer()),
    ('classifier', [YOUR_CLASSIFIER])
])
```

**Hyperparameters:**
- **CountVectorizer:**
  - `min_df=5`: Ignore terms appearing in <5 documents
  - `ngram_range=(1,2)`: Use unigrams and bigrams
- **Random Forest:** 50 estimators
- **Gradient Boosting:** 150 estimators, max_depth=6

### Approach 2: SMOTE-Enhanced ML

**Pipeline Configuration:**
```python
imbpipeline([
    ('vect', CountVectorizer(min_df=5, ngram_range=(1, 2))),
    ('tfidf', TfidfTransformer()),
    ('smote', SMOTE()),  # Synthetic oversampling
    ('classifier', [YOUR_CLASSIFIER])
])
```

**Why SMOTE?**
- Dataset has ~87% ham, ~13% spam (imbalanced)
- SMOTE generates synthetic minority samples
- Improves model generalization on spam class

### Approach 3: BERT Transformer

**Architecture:**
```python
BERT Base Model (Hugging Face)
    â†“
BERT Tokenizer (WordPiece)
    â†“
[CLS] token embedding (768-dim)
    â†“
Dense Layer (Binary Classification)
    â†“
Sigmoid Activation
```

**Advantages:**
- Contextual word embeddings
- Bidirectional understanding
- Pre-trained on massive corpus
- Transfer learning benefits

## ğŸ“ˆ Results & Analysis

### Performance Comparison

| Model | Training Time | Inference Speed | Memory | Best For |
|-------|--------------|-----------------|---------|----------|
| **LinearSVC** | âš¡ Fast | âš¡âš¡âš¡ Very Fast | ğŸ’¾ Low | Production |
| **SGD** | âš¡âš¡ Very Fast | âš¡âš¡âš¡ Very Fast | ğŸ’¾ Low | Large datasets |
| **Random Forest** | âš¡ Moderate | âš¡âš¡ Fast | ğŸ’¾ Medium | Interpretability |
| **BERT** | ğŸŒ Slow | âš¡ Moderate | ğŸ’¾ğŸ’¾ High | Complex text |

### Key Findings

1. âœ… **Traditional ML Effectiveness**: LinearSVC and SGD matched BERT's 98% accuracy
2. âœ… **Speed Advantage**: Traditional models are 10-100x faster for inference
3. âœ… **SMOTE Impact**: Improved recall on minority (spam) class by 3-5%
4. âœ… **BERT Strengths**: Better handles context, sarcasm, and nuanced language
5. âœ… **Production Recommendation**: LinearSVC for speed; BERT for accuracy on edge cases

### Confusion Matrix Analysis

**LinearSVC Performance:**
```
                Predicted
              Ham    Spam
Actual Ham    [947]   [12]
Actual Spam   [8]     [148]
```
- Precision (Spam): 92.5%
- Recall (Spam): 94.9%
- F1-Score: 93.7%

## ğŸ“š Dataset

<div align="center">

**[SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)**

</div>

| Attribute | Details |
|-----------|---------|
| **Source** | UCI Machine Learning Repository |
| **Total Messages** | 5,574 SMS |
| **Language** | English |
| **Classes** | Ham (Legitimate) / Spam |
| **Format** | CSV with labeled data |
| **Class Distribution** | Ham: 86.6% / Spam: 13.4% |

**Sample Messages:**

| Type | Example |
|------|---------|
| ğŸ“§ **Ham** | "Ok lar... Joking wif u oni..." |
| ğŸ“§ **Ham** | "What you doing?how are you?" |
| ğŸš« **Spam** | "FREE for 1st week! No1 Nokia tone 4 ur mob..." |
| ğŸš« **Spam** | "WINNER!! You have won a Â£1000 cash prize!" |

## ğŸ› ï¸ Tech Stack

<div align="center">

| Category | Technologies |
|----------|-------------|
| **Language** | ![Python](https://img.shields.io/badge/Python-3.6+-3776AB?logo=python&logoColor=white) |
| **Data Processing** | Pandas 1.0.5 â€¢ NumPy 1.16.4 |
| **Machine Learning** | Scikit-learn 0.23.2 â€¢ Imbalanced-learn 0.5.0 |
| **NLP** | NLTK 3.4.5 â€¢ Transformers (Hugging Face) |
| **Deep Learning** | TensorFlow â€¢ Keras |
| **Visualization** | Matplotlib 3.1.1 â€¢ Seaborn 0.9.0 |
| **Model Persistence** | Joblib |

</div>

## ğŸ“ Learning Outcomes

By exploring this project, you'll master:

- âœ… Text preprocessing and feature engineering for NLP
- âœ… Multiple ML classification algorithms and comparison
- âœ… Handling imbalanced datasets with SMOTE
- âœ… Implementing and fine-tuning BERT transformers
- âœ… Building production-ready ML pipelines
- âœ… Model evaluation and selection strategies
- âœ… Deploying trained models for inference

## ğŸ’¡ Use Cases & Applications

This spam detection system can be adapted for:

- ğŸ“± **SMS Filtering**: Mobile carriers and messaging apps
- ğŸ“§ **Email Security**: Spam and phishing detection
- ğŸ’¬ **Chat Moderation**: Social media and forums
- ğŸ›¡ï¸ **Fraud Prevention**: Financial institutions
- ğŸ“ **Call Center**: Automated message triage
- ğŸ¤– **Chatbots**: Filter malicious inputs

## ğŸ”® Future Enhancements

- [ ] ğŸŒ REST API with Flask/FastAPI
- [ ] ğŸ¨ Interactive web UI (Streamlit/Gradio)
- [ ] ğŸ“± Mobile app integration (iOS/Android)
- [ ] ğŸŒ Multi-language support (Spanish, French, etc.)
- [ ] ğŸ”„ Ensemble methods (ML + BERT hybrid)
- [ ] ğŸ“Š Real-time monitoring dashboard
- [ ] ğŸ³ Docker containerization
- [ ] â˜ï¸ Cloud deployment (AWS SageMaker, GCP AI Platform)
- [ ] ğŸ”„ Active learning pipeline
- [ ] ğŸ“ˆ A/B testing framework

## ğŸ› Troubleshooting

### Common Issues

**Issue: NLTK data not found**
```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```

**Issue: File path errors on Windows**
```python
# Change backslashes to forward slashes
URL_DATA = 'data/spam.csv'  # âœ… Works on all platforms
```

**Issue: Memory error with BERT**
```python
# Reduce batch size or use a smaller model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
```

**Issue: Model file not found**
```bash
# Ensure models/ directory exists
mkdir models
# Train model first before running predictions.py
python spam_model.py
```

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. ğŸ´ **Fork** the repository
2. ğŸŒ¿ **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ **Commit** your changes (`git commit -m 'Add AmazingFeature'`)
4. ğŸ“¤ **Push** to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ”€ **Open** a Pull Request

### Areas for Contribution
- ğŸ†• Additional ML algorithms (XGBoost, LightGBM)
- ğŸ¯ Hyperparameter optimization (GridSearch, Optuna)
- ğŸ“Š More visualization and EDA
- ğŸ§ª Unit tests and CI/CD
- ğŸ“ Documentation improvements
- ğŸŒ API development
- ğŸ¨ UI/UX enhancements

## ğŸ“œ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¨â€ğŸ’» Author

**Your Name**

- ğŸ™ GitHub: [@yourusername](https://github.com/yourusername)
- ğŸ’¼ LinkedIn: [Your Profile](https://linkedin.com/in/yourprofile)
- ğŸ“§ Email: your.email@example.com
- ğŸŒ Portfolio: [yourwebsite.com](https://yourwebsite.com)

## ğŸ™ Acknowledgments

- **UCI ML Repository** for the SMS Spam Collection dataset
- **Hugging Face** for BERT and Transformers library
- **NLTK Team** for natural language processing tools
- **Scikit-learn** community for ML algorithms
- **Imbalanced-learn** for SMOTE implementation

## ğŸ“š References

- [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [SMOTE: Synthetic Minority Over-sampling Technique](https://arxiv.org/abs/1106.1813)
- [Scikit-learn Documentation](https://scikit-learn.org/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)

## â­ Show Your Support

If this project helped you or you found it interesting:

- â­ **Star** this repository
- ğŸ¦ **Share** on social media
- ğŸ’¬ **Provide feedback** via Issues
- ğŸ´ **Fork** and build something cool!
- ğŸ“ **Write** a blog post about it

---

<div align="center">

**Made with â¤ï¸ and Python**

*Comparing traditional ML and modern transformers for real-world NLP*

![Visitor Count](https://visitor-badge.laobi.icu/badge?page_id=yourusername.spam-detection)

[â¬† Back to Top](#-sms-spam-detection-ml-vs-bert-showdown)

</div>
